{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EarlyStopping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 145\u001b[0m\n\u001b[1;32m    125\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#-------------------\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Entrainement\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#-------------------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# D√®s que val_loss cesse de s'am√©liorer pendant 10 √©poques cons√©cutives,\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# le training s'arr√™te automatiquement, et les meilleurs poids sont restaur√©s.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m \u001b[43mEarlyStopping\u001b[49m(\n\u001b[1;32m    146\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    147\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    148\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    150\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stop],verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m#-------------------\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Pr√©diction\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m#-------------------\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# on lance la pr√©diction y_pred avec X_test_scaled valeur nouvelle\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EarlyStopping' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Manipulation de donn√©es tabulaires (exel,csv) sous forme de tableaux dataframes\n",
    "import numpy as np  #calcul num√©rique rapides sur des tableaux (arrays)\n",
    "import matplotlib.pyplot as plt #visualisation graphiques...\n",
    "import seaborn as sns #faire des graphique s statistiques jolis et simple\n",
    "\n",
    "from sklearn.model_selection import train_test_split #entrainer, tester et evaluer des modeles de ML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import tensorflow as tf # construction et entrainement de r√©seaux de neurones profonds\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Charge le dataset / constitution du dataframe / et suppression des espaces inutiles + passage en minuscule \n",
    "path = \"/Users/amar/Desktop/Alyra_School/Alyra 2025/4- Projet/investments_VC.csv\"\n",
    "\n",
    "#------------\n",
    "# NETTOYAGE ET PREPARATION DES DONNEES \n",
    "#------------\n",
    "\n",
    "df = pd.read_csv(path, encoding='unicode_escape')\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Colonnes utiles\n",
    "cols = ['country_code', 'market', 'category_list', 'funding_rounds', 'founded_year', 'funding_total_usd']\n",
    "df = df[cols]\n",
    "\n",
    "# Nettoyage des montants\n",
    "df['funding_total_usd'] = df['funding_total_usd'].replace(r'[\\$,]', '', regex=True) # Suppression les caract√®res $ et , dans les montants\n",
    "df['funding_total_usd'] = pd.to_numeric(df['funding_total_usd'], errors='coerce')   # Conversion de la colonne en float. Les erreurs de conversion deviennent des NaN\n",
    "\n",
    "# Nettoyage : suppression des NaN et des montants √† 0\n",
    "df.dropna(inplace=True)                 # Suppression de toutes les lignes contenant des valeurs manquantes sur ces colonnes (country_code, market, category_list, funding_rounds, founded_year, funding_total_usd)\n",
    "df = df[df['funding_total_usd'] > 0]    #suprresion des lignes avec des montants de financement nuls ou n√©gatifs.\n",
    "\n",
    "# Transformation de la cible en log\n",
    "df['log_funding'] = np.log1p(df['funding_total_usd'])   # on applique log1p (log(1+x) pour r√©duire les √©carts de valeurs extremes\n",
    "\n",
    "# Encodage des variables cat√©gorielles  \n",
    "# Drop(..): supprime les colonnes 'funding_total_usd' (montant r√©el--> version brut d√©j√† transform√©e), 'log_funding' (cible de la pr√©diction--> variable cible (y))\n",
    "# pd.get.dummies(..., drop first=True) : transforme les colonnes cat√©gorielles(texte) en colonnes num√©riques grace √† l'encodage one-hot.\n",
    "#suppression de la premi√®re modalit√© de chaque colonne encod√©e pour √©viter les redondance (multicolin√©arit√©) \n",
    "X = pd.get_dummies(df.drop(['funding_total_usd', 'log_funding'], axis=1), drop_first=True)\n",
    "\n",
    "# Cible\n",
    "y = df['log_funding']   #variable cible de l'apprentissage\n",
    "\n",
    "# Split train/test (scikit-learn)\n",
    "# X_train : les features utilis√©es pour entra√Æner le mod√®le\n",
    "# X_test : les features utilis√©es pour tester le mod√®le\n",
    "# y_train : les valeurs cibles correspondantes √† X_train\n",
    "# y_test : les valeurs cibles correspondantes √† X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardisation\n",
    "scaler = StandardScaler()\n",
    "# On cr√©e un objet StandardScaler de sklearn.\n",
    "# Cet outil permet de standardiser les donn√©es, c‚Äôest-√†-dire de les transformer pour qu‚Äôelles aient :\n",
    "# une moyenne (mean) de 0\n",
    "# un √©cart-type (std) de 1\n",
    "# -> Cela facilite l'entra√Ænement car les variables ont des ordres de grandeur comparables.\n",
    "\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# Calculer la moyenne et l‚Äô√©cart-type des colonnes de X_train (c‚Äôest le .fit())\n",
    "# Appliquer la transformation (c‚Äôest le .transform())\n",
    "# R√©sultat : chaque colonne de X_train_scaled est centr√©e r√©duite.\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# On applique la m√™me transformation aux donn√©es de test X_test.\n",
    "# on n‚Äôutilise pas .fit_transform() sur X_test, car on veut garder les m√™mes param√®tres (moyenne et √©cart-type) que ceux appris sur les donn√©es d'entra√Ænement.\n",
    "\n",
    "# Attention : \n",
    "# fit() = apprend les statistiques (moyenne, √©cart-type)\n",
    "# transform() = applique la transformation (x - ¬µ) / œÉ aux donn√©es\n",
    "\n",
    "#-------------------\n",
    "# Construction du modele de neurones en utilsant la bibliotheque Keras(Tensorflow)\n",
    "#-------------------\n",
    "\n",
    "# L‚Äôobjectif est de pr√©dire un montant (logarithm√©), donc la sortie est une valeur continue.\n",
    "# Le modele est compos√© de 3 couches de neurones Denses, entrecoup√© de 2 couches de dropout (cach√©es) avec activation ReLU\n",
    "\n",
    "# Il y a 2 couches  dense avec activation ReLU\n",
    "# des couches Dropout pour √©viter le surapprentissage (Overfifting)\n",
    "# une sortie avec 1 seule valeur\n",
    "# une compilation avec l'optimiseur Adam et une fonction de perte MSE (erreur quadratique moyenne)\n",
    "\n",
    "# Cr√©ation d‚Äôun mod√®le s√©quentiel, c‚Äôest-√†-dire un empilement de couches lin√©aires les unes apr√®s les autres.\n",
    "model = Sequential()\n",
    "\n",
    "# Ajout 1ere couche Dense (fully connected) de 128 neurones avec activation ReLU(Rectified Linear Unit) pour introduire de la non lin√©arit√©\n",
    "# La fonction ReLU transforme une valeur en elle-m√™me si elle est positive, ou en 0 si elle est n√©gative.\n",
    "# Elle permet au r√©seau de mieux apprendre des fonctions complexes tout en restant tr√®s efficace √† l'entra√Ænement.\n",
    "\n",
    "# input_dim=X_train_scaled.shape[1] : dimension d‚Äôentr√©e = nombre de features (colonnes) dans X_train_scaled.\n",
    "\n",
    "# ReLU(x)={ x si¬†x>0 (une¬†droite) Sinon =0 si x‚â§0(une¬†constante)\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
    "\n",
    "# Ajout d'une couche Dropout : pendant l'entrainement 30% des neurones sont d√©sactiv√©s al√©atoirement pour √©viter le surapprentissage(r√©gularisation)\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Cr√©ation de 2ieme couche Denses de 64 neurones + ReLU pour la non-lin√©arit√©\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1))  # 1 seul neurone, pas de fonction d‚Äôactivation \n",
    "\n",
    "# Configuration du modele avant entrainement pour apprendre en minimisant l‚Äôerreur quadratique moyenne, √† l‚Äôaide de l‚Äôoptimiseur Adam.\n",
    "\n",
    "# Ce modele dit comment le modele va apprendre (l'optimiseur) et ce qu'il doit minimiser (fonction de perte)\n",
    "# Adam : algo d'optimisation -> il ajuste le taux d'apprentissage a chaque it√©ration (ajuste les poids pour r√©duire l'erreur : rapide, efficace, n√©cessite peu de r√©glages manuels)\n",
    "    # Adam = Adaptive Moment Estimation : MAJ des poids du r√©seau (w) pendant l'apprentissage\n",
    "        # Adam = Momentum + RMSProp\n",
    "            # Momentum : il garde en m√©moire une moyenne des anciennes directions comme une inertie\n",
    "            # RMSProp : Root Mean Square Propagation - adapte automatiquement la taille des pas pour chaque poids.\n",
    "                # Le pas = quantit√© de changement appliqu√©e au poids du modele √† chaque mise √† jour. D√©termine a quelle vitesse le modele apprend.\n",
    "                # il d√©pend du gradient : direction √† suivre pour r√©duire l'erreur\n",
    "                # et du Learning rate : determine la taille du pas\n",
    "\n",
    "# loss='mse -> Mean square error (erreur quadratique moyenne)= fonction de perte utilis√© pour les prb de r√©gression\n",
    "    # --> calcul la moyenne des carr√©s de √©carts entre les pr√©dictions du modele et les vraies valeurs \n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "#-------------------\n",
    "# Entrainement\n",
    "#-------------------\n",
    "\n",
    "# Entra√Ænement en utilisant les donn√©es d'entr√©es X_train_scaled et les cibles y_train\n",
    "# X_train_scaled : les donn√©es d‚Äôentr√©e (features), d√©j√† normalis√©es. \n",
    "# y_train : les valeurs cibles qu‚Äôon veut pr√©dire.\n",
    "# epochs=100 : Le mod√®le va parcourir toutes les donn√©es d'entra√Ænement 100 fois. Chaque passage complet sur l‚Äôensemble s‚Äôappelle une \"√©poque\".\n",
    "# batch_size=32: les donn√©es sont divis√©es en petits groupes de 32 exemples.\n",
    "# Cela r√©serve automatiquement 20% des donn√©es d‚Äôentra√Ænement pour valider le mod√®le √† chaque √©poque.\n",
    "# verbose=0 : Mode silence \n",
    "#l'objet history contient l'√©volution des m√©trics √† chaque √©poque. \n",
    "# history.history['loss'] : erreur d'entra√Ænement\n",
    "# history.history['val_loss'] : erreur de validation\n",
    "\n",
    "# D√®s que val_loss cesse de s'am√©liorer pendant 10 √©poques cons√©cutives,\n",
    "# le training s'arr√™te automatiquement, et les meilleurs poids sont restaur√©s.\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stop],verbose=0)\n",
    "\n",
    "#-------------------\n",
    "# Pr√©diction\n",
    "#-------------------\n",
    "# on lance la pr√©diction y_pred avec X_test_scaled valeur nouvelle\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "#-------------------\n",
    "# √âvaluation\n",
    "#-------------------\n",
    "\n",
    "# On √©value l‚Äôerreur entre la v√©rit√© (y_test) et ce que le mod√®le a pr√©dit (y_pred).\n",
    "# mean_squared_error() = moyenne des carr√©s des √©carts ‚Üí mesure l‚Äôerreur moyenne.\n",
    "#L'erreur moyenne est de \n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# arrive √† pr√©dire 29,8% des fluctuations\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ RMSE : {rmse}\")\n",
    "print(f\"‚úÖ R¬≤ score : {r2}\")\n",
    "\n",
    "# üîπ Visualisation\n",
    "\n",
    "# Tracer la courbe de perte (training vs validation)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Erreur entra√Ænement')\n",
    "plt.plot(history.history['val_loss'], label='Erreur validation')\n",
    "plt.xlabel('√âpoques')\n",
    "plt.ylabel('Erreur (loss)')\n",
    "plt.title('√âvolution de l‚Äôerreur pendant l‚Äôentra√Ænement')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Avec 100 epoques \n",
    "# üîµ Erreur entra√Ænement (ligne bleue) :\n",
    "# Elle diminue r√©guli√®rement et reste tr√®s basse.\n",
    "# Le mod√®le apprend tr√®s bien les donn√©es d‚Äôentra√Ænement.\n",
    "\n",
    "# üü† Erreur validation (ligne orange) :\n",
    "# Elle commence correctement (basse au d√©part), puis :\n",
    "# Elle explose √† partir d‚Äôenviron 15-20 √©poques, avec de fortes variations (instabilit√©).\n",
    "# Le mod√®le n‚Äôarrive plus √† g√©n√©raliser.\n",
    "# Il est clairement en overfitting.\n",
    "#>>> Ajout d'un early stop\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred.flatten(), alpha=0.5) # x-> valeur r√©elles et y-> valeurs pr√©dites\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\n",
    "plt.xlabel(\"Valeurs r√©elles (log)\")\n",
    "plt.ylabel(\"Pr√©dictions (log)\")\n",
    "plt.title(\"Pr√©diction avec Deep Learning\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

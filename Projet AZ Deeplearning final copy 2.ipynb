{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EarlyStopping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 145\u001b[0m\n\u001b[1;32m    125\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#-------------------\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Entrainement\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#-------------------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Dès que val_loss cesse de s'améliorer pendant 10 époques consécutives,\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# le training s'arrête automatiquement, et les meilleurs poids sont restaurés.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m \u001b[43mEarlyStopping\u001b[49m(\n\u001b[1;32m    146\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    147\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    148\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    150\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stop],verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m#-------------------\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Prédiction\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m#-------------------\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# on lance la prédiction y_pred avec X_test_scaled valeur nouvelle\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EarlyStopping' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Manipulation de données tabulaires (exel,csv) sous forme de tableaux dataframes\n",
    "import numpy as np  #calcul numérique rapides sur des tableaux (arrays)\n",
    "import matplotlib.pyplot as plt #visualisation graphiques...\n",
    "import seaborn as sns #faire des graphique s statistiques jolis et simple\n",
    "\n",
    "from sklearn.model_selection import train_test_split #entrainer, tester et evaluer des modeles de ML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import tensorflow as tf # construction et entrainement de réseaux de neurones profonds\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Charge le dataset / constitution du dataframe / et suppression des espaces inutiles + passage en minuscule \n",
    "path = \"/Users/amar/Desktop/Alyra_School/Alyra 2025/4- Projet/investments_VC.csv\"\n",
    "\n",
    "#------------\n",
    "# NETTOYAGE ET PREPARATION DES DONNEES \n",
    "#------------\n",
    "\n",
    "df = pd.read_csv(path, encoding='unicode_escape')\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Colonnes utiles\n",
    "cols = ['country_code', 'market', 'category_list', 'funding_rounds', 'founded_year', 'funding_total_usd']\n",
    "df = df[cols]\n",
    "\n",
    "# Nettoyage des montants\n",
    "df['funding_total_usd'] = df['funding_total_usd'].replace(r'[\\$,]', '', regex=True) # Suppression les caractères $ et , dans les montants\n",
    "df['funding_total_usd'] = pd.to_numeric(df['funding_total_usd'], errors='coerce')   # Conversion de la colonne en float. Les erreurs de conversion deviennent des NaN\n",
    "\n",
    "# Nettoyage : suppression des NaN et des montants à 0\n",
    "df.dropna(inplace=True)                 # Suppression de toutes les lignes contenant des valeurs manquantes sur ces colonnes (country_code, market, category_list, funding_rounds, founded_year, funding_total_usd)\n",
    "df = df[df['funding_total_usd'] > 0]    #suprresion des lignes avec des montants de financement nuls ou négatifs.\n",
    "\n",
    "# Transformation de la cible en log\n",
    "df['log_funding'] = np.log1p(df['funding_total_usd'])   # on applique log1p (log(1+x) pour réduire les écarts de valeurs extremes\n",
    "\n",
    "# Encodage des variables catégorielles  \n",
    "# Drop(..): supprime les colonnes 'funding_total_usd' (montant réel--> version brut déjà transformée), 'log_funding' (cible de la prédiction--> variable cible (y))\n",
    "# pd.get.dummies(..., drop first=True) : transforme les colonnes catégorielles(texte) en colonnes numériques grace à l'encodage one-hot.\n",
    "#suppression de la première modalité de chaque colonne encodée pour éviter les redondance (multicolinéarité) \n",
    "X = pd.get_dummies(df.drop(['funding_total_usd', 'log_funding'], axis=1), drop_first=True)\n",
    "\n",
    "# Cible\n",
    "y = df['log_funding']   #variable cible de l'apprentissage\n",
    "\n",
    "# Split train/test (scikit-learn)\n",
    "# X_train : les features utilisées pour entraîner le modèle\n",
    "# X_test : les features utilisées pour tester le modèle\n",
    "# y_train : les valeurs cibles correspondantes à X_train\n",
    "# y_test : les valeurs cibles correspondantes à X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardisation\n",
    "scaler = StandardScaler()\n",
    "# On crée un objet StandardScaler de sklearn.\n",
    "# Cet outil permet de standardiser les données, c’est-à-dire de les transformer pour qu’elles aient :\n",
    "# une moyenne (mean) de 0\n",
    "# un écart-type (std) de 1\n",
    "# -> Cela facilite l'entraînement car les variables ont des ordres de grandeur comparables.\n",
    "\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# Calculer la moyenne et l’écart-type des colonnes de X_train (c’est le .fit())\n",
    "# Appliquer la transformation (c’est le .transform())\n",
    "# Résultat : chaque colonne de X_train_scaled est centrée réduite.\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# On applique la même transformation aux données de test X_test.\n",
    "# on n’utilise pas .fit_transform() sur X_test, car on veut garder les mêmes paramètres (moyenne et écart-type) que ceux appris sur les données d'entraînement.\n",
    "\n",
    "# Attention : \n",
    "# fit() = apprend les statistiques (moyenne, écart-type)\n",
    "# transform() = applique la transformation (x - µ) / σ aux données\n",
    "\n",
    "#-------------------\n",
    "# Construction du modele de neurones en utilsant la bibliotheque Keras(Tensorflow)\n",
    "#-------------------\n",
    "\n",
    "# L’objectif est de prédire un montant (logarithmé), donc la sortie est une valeur continue.\n",
    "# Le modele est composé de 3 couches de neurones Denses, entrecoupé de 2 couches de dropout (cachées) avec activation ReLU\n",
    "\n",
    "# Il y a 2 couches  dense avec activation ReLU\n",
    "# des couches Dropout pour éviter le surapprentissage (Overfifting)\n",
    "# une sortie avec 1 seule valeur\n",
    "# une compilation avec l'optimiseur Adam et une fonction de perte MSE (erreur quadratique moyenne)\n",
    "\n",
    "# Création d’un modèle séquentiel, c’est-à-dire un empilement de couches linéaires les unes après les autres.\n",
    "model = Sequential()\n",
    "\n",
    "# Ajout 1ere couche Dense (fully connected) de 128 neurones avec activation ReLU(Rectified Linear Unit) pour introduire de la non linéarité\n",
    "# La fonction ReLU transforme une valeur en elle-même si elle est positive, ou en 0 si elle est négative.\n",
    "# Elle permet au réseau de mieux apprendre des fonctions complexes tout en restant très efficace à l'entraînement.\n",
    "\n",
    "# input_dim=X_train_scaled.shape[1] : dimension d’entrée = nombre de features (colonnes) dans X_train_scaled.\n",
    "\n",
    "# ReLU(x)={ x si x>0 (une droite) Sinon =0 si x≤0(une constante)\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
    "\n",
    "# Ajout d'une couche Dropout : pendant l'entrainement 30% des neurones sont désactivés aléatoirement pour éviter le surapprentissage(régularisation)\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Création de 2ieme couche Denses de 64 neurones + ReLU pour la non-linéarité\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1))  # 1 seul neurone, pas de fonction d’activation \n",
    "\n",
    "# Configuration du modele avant entrainement pour apprendre en minimisant l’erreur quadratique moyenne, à l’aide de l’optimiseur Adam.\n",
    "\n",
    "# Ce modele dit comment le modele va apprendre (l'optimiseur) et ce qu'il doit minimiser (fonction de perte)\n",
    "# Adam : algo d'optimisation -> il ajuste le taux d'apprentissage a chaque itération (ajuste les poids pour réduire l'erreur : rapide, efficace, nécessite peu de réglages manuels)\n",
    "    # Adam = Adaptive Moment Estimation : MAJ des poids du réseau (w) pendant l'apprentissage\n",
    "        # Adam = Momentum + RMSProp\n",
    "            # Momentum : il garde en mémoire une moyenne des anciennes directions comme une inertie\n",
    "            # RMSProp : Root Mean Square Propagation - adapte automatiquement la taille des pas pour chaque poids.\n",
    "                # Le pas = quantité de changement appliquée au poids du modele à chaque mise à jour. Détermine a quelle vitesse le modele apprend.\n",
    "                # il dépend du gradient : direction à suivre pour réduire l'erreur\n",
    "                # et du Learning rate : determine la taille du pas\n",
    "\n",
    "# loss='mse -> Mean square error (erreur quadratique moyenne)= fonction de perte utilisé pour les prb de régression\n",
    "    # --> calcul la moyenne des carrés de écarts entre les prédictions du modele et les vraies valeurs \n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "#-------------------\n",
    "# Entrainement\n",
    "#-------------------\n",
    "\n",
    "# Entraînement en utilisant les données d'entrées X_train_scaled et les cibles y_train\n",
    "# X_train_scaled : les données d’entrée (features), déjà normalisées. \n",
    "# y_train : les valeurs cibles qu’on veut prédire.\n",
    "# epochs=100 : Le modèle va parcourir toutes les données d'entraînement 100 fois. Chaque passage complet sur l’ensemble s’appelle une \"époque\".\n",
    "# batch_size=32: les données sont divisées en petits groupes de 32 exemples.\n",
    "# Cela réserve automatiquement 20% des données d’entraînement pour valider le modèle à chaque époque.\n",
    "# verbose=0 : Mode silence \n",
    "#l'objet history contient l'évolution des métrics à chaque époque. \n",
    "# history.history['loss'] : erreur d'entraînement\n",
    "# history.history['val_loss'] : erreur de validation\n",
    "\n",
    "# Dès que val_loss cesse de s'améliorer pendant 10 époques consécutives,\n",
    "# le training s'arrête automatiquement, et les meilleurs poids sont restaurés.\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stop],verbose=0)\n",
    "\n",
    "#-------------------\n",
    "# Prédiction\n",
    "#-------------------\n",
    "# on lance la prédiction y_pred avec X_test_scaled valeur nouvelle\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "#-------------------\n",
    "# Évaluation\n",
    "#-------------------\n",
    "\n",
    "# On évalue l’erreur entre la vérité (y_test) et ce que le modèle a prédit (y_pred).\n",
    "# mean_squared_error() = moyenne des carrés des écarts → mesure l’erreur moyenne.\n",
    "#L'erreur moyenne est de \n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# arrive à prédire 29,8% des fluctuations\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"✅ RMSE : {rmse}\")\n",
    "print(f\"✅ R² score : {r2}\")\n",
    "\n",
    "# 🔹 Visualisation\n",
    "\n",
    "# Tracer la courbe de perte (training vs validation)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Erreur entraînement')\n",
    "plt.plot(history.history['val_loss'], label='Erreur validation')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Erreur (loss)')\n",
    "plt.title('Évolution de l’erreur pendant l’entraînement')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Avec 100 epoques \n",
    "# 🔵 Erreur entraînement (ligne bleue) :\n",
    "# Elle diminue régulièrement et reste très basse.\n",
    "# Le modèle apprend très bien les données d’entraînement.\n",
    "\n",
    "# 🟠 Erreur validation (ligne orange) :\n",
    "# Elle commence correctement (basse au départ), puis :\n",
    "# Elle explose à partir d’environ 15-20 époques, avec de fortes variations (instabilité).\n",
    "# Le modèle n’arrive plus à généraliser.\n",
    "# Il est clairement en overfitting.\n",
    "#>>> Ajout d'un early stop\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred.flatten(), alpha=0.5) # x-> valeur réelles et y-> valeurs prédites\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\n",
    "plt.xlabel(\"Valeurs réelles (log)\")\n",
    "plt.ylabel(\"Prédictions (log)\")\n",
    "plt.title(\"Prédiction avec Deep Learning\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
